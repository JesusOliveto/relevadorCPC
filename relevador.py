# -*- coding: utf-8 -*-
"""
Relevador CPC (ComunicaciÃ³n PÃºblica de la Ciencia)
- Motor: Google Custom Search JSON API (Programmable Search Engine).
- CategorÃ­as: Ciencia Abierta, ComunicaciÃ³n PÃºblica de la Ciencia, Diplomacia CientÃ­fica.
- Evita PDF/binarios en "Contenido Muestra" y sanea todo antes de exportar a Excel.
"""

import os
import io
import re
import time
import random
import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from datetime import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Relevador CPC - Buscador de Universidades",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Credenciales (Secrets/Entorno) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_KEY = (
    (st.secrets.get("GOOGLE_API_KEY") if hasattr(st, "secrets") else None)
    or os.getenv("GOOGLE_API_KEY")
)
CSE_CX = (
    (st.secrets.get("GOOGLE_CSE_CX") if hasattr(st, "secrets") else None)
    or os.getenv("GOOGLE_CSE_CX")
)
CSE_PUBLIC_URL = "https://cse.google.com/cse?cx=d15e41407a20a49c6"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Constantes/Vocabularios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REQ_TIMEOUT = 12
HEADERS_POOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
]
def _headers():
    return {"User-Agent": random.choice(HEADERS_POOL), "Accept-Language": "es-AR,es;q=0.9,en;q=0.8"}

TERMINOS_BUSQUEDA = {
    "ciencia_abierta": [
        '("open science" OR "open data" OR "open access") university',
        "universidad ciencia abierta",
        "universitat ciÃ¨ncia oberta",
        "universitÃ© science ouverte",
        "universidade ciÃªncia aberta",
        "universitÃ  scienza aperta",
    ],
    "comunicacion_publica": [
        '("science communication" OR "public engagement") university',
        "universidad comunicaciÃ³n pÃºblica de la ciencia",
        "universitat comunicaciÃ³ cientÃ­fica",
        "universitÃ© communication scientifique",
        "universidade comunicaÃ§Ã£o cientÃ­fica",
    ],
    "diplomacia_cientifica": [
        '("science diplomacy") university',
        "universidad diplomacia cientÃ­fica",
        "universitat diplomÃ cia cientÃ­fica",
        "universitÃ© diplomatie scientifique",
        "universidade diplomacia cientÃ­fica",
    ],
}

TERMINOS_VALIDACION = {
    "ciencia_abierta": [
        "open science","ciencia abierta","ciÃ¨ncia oberta","ciÃªncia aberta","science ouverte",
        "open data","datos abiertos","dades obertes","dados abertos","donnÃ©es ouvertes",
        "open access","acceso abierto","accÃ©s obert","acesso aberto","accÃ¨s libre",
        "fair data","repositorio institucional","institutional repository","reproducible research",
    ],
    "comunicacion_publica": [
        "science communication","comunicaciÃ³n cientÃ­fica","comunicaciÃ³n pÃºblica de la ciencia",
        "public engagement","outreach","vulgarisation scientifique","divulgaÃ§Ã£o cientÃ­fica",
        "science literacy","public understanding of science","cultura cientÃ­fica",
    ],
    "diplomacia_cientifica": [
        "science diplomacy","diplomacia cientÃ­fica","diplomÃ cia cientÃ­fica","diplomatie scientifique",
        "international scientific cooperation","cooperaciÃ³n internacional cientÃ­fica",
        "science policy","polÃ­tica cientÃ­fica",
    ],
}

BAD_DOMAINS_SUBSTR = [
    "wikipedia.org","wikidata.org","web.archive.org",
    "blogspot.","wordpress.","medium.com",
    "topuniversities.","timeshighereducation.","theworlduniversityrankings",
    "4icu.org","uni-rank","edurank","shanghairanking",
    "qs.com","mastersportal","bachelorstudies","studocu","prezi",
    "/ranking","/rankings","/directory","/directorio",
]

CONTROL = {
    "nombre": "Universitat Jaume I",
    "url": "https://www.uji.es/",
    "pais": "EspaÃ±a",
    "idioma": "CatalÃ¡n/EspaÃ±ol",
    "buscar_refuerzo": ['site:uji.es ("ciÃ¨ncia oberta" OR "ciencia abierta" OR "open science")'],
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helpers: Excel sanitization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ILLEGAL_XLSX_RE = re.compile(r"[\x00-\x08\x0B\x0C\x0E-\x1F]")

def sanitize_excel_value(v):
    if v is None:
        return ""
    if not isinstance(v, str):
        v = str(v)
    v = ILLEGAL_XLSX_RE.sub("", v)
    if len(v) > 32767:
        v = v[:32760] + "..."
    return v

def sanitize_dataframe_for_excel(df: pd.DataFrame) -> pd.DataFrame:
    for col in df.columns:
        df[col] = df[col].map(sanitize_excel_value)
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helpers web â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False, ttl=24*3600)
def http_get(url: str):
    """
    Devuelve: (status_code, text_or_empty, content_type)
    Solo retorna texto si la respuesta es HTML/texto/XML. Para PDF/binarios deja texto = "".
    """
    try:
        r = requests.get(url, headers=_headers(), timeout=REQ_TIMEOUT)
        ct = (r.headers.get("Content-Type") or "").lower()
        if "html" in ct or ct.startswith("text/") or "xml" in ct:
            return r.status_code, r.text, ct
        return r.status_code, "", ct
    except Exception:
        return 0, "", ""

def clean_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for bad in soup(["script","style","noscript","header","footer","nav"]):
        bad.decompose()
    text = soup.get_text(" ")
    return re.sub(r"\s+", " ", text).strip()

def same_domain(a: str, b: str) -> bool:
    try:
        return urlparse(a).netloc == urlparse(b).netloc
    except:
        return False

def is_university(url: str, title: str = "") -> bool:
    if not url:
        return False
    u = url.lower()
    if any(b in u for b in BAD_DOMAINS_SUBSTR):
        return False
    strong = any(re.search(p, u) for p in [
        r"\.edu($|/)", r"\.edu\.[a-z]+$", r"\.ac\.", r"\.ac$", r"\.uni\.", r"\.univ\.", r"\.edu\.", r"\.ac\.[a-z]+$"
    ])
    if strong:
        return True
    t = (title or "").lower()
    words = ["university","universidad","universitat","universidade","universitÃ©","universitÃ ","college","instituto","institute"]
    if any(w in t for w in words) and not any(x in t for x in ["ranking","rankings","list","directorio","directory","top"]):
        return True
    host = urlparse(url).netloc.lower()
    if host.startswith("uni.") or host.startswith("univ."):
        return True
    return False

def country_guess(domain: str) -> str:
    m = {
        ".es":"EspaÃ±a",".edu":"Estados Unidos",".uk":"Reino Unido",".ca":"CanadÃ¡",".au":"Australia",".de":"Alemania",
        ".fr":"Francia",".it":"Italia",".br":"Brasil",".ar":"Argentina",".mx":"MÃ©xico",".cl":"Chile",".co":"Colombia",
        ".pe":"PerÃº",".jp":"JapÃ³n",".cn":"China",".in":"India",".nl":"PaÃ­ses Bajos",".ch":"Suiza",".se":"Suecia",".no":"Noruega"
    }
    for ext, pais in m.items():
        if domain.endswith(ext): return pais
    return "Internacional"

def find_relevant_links(base_url: str, html: str, limit: int = 12) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")
    keys = [
        "research","investigaciÃ³n","investigaciÃ³","pesquisa","recherche","ricerca",
        "science","ciencia","ciÃ¨ncia","ciÃªncia","scienza",
        "open","abierto","obert","aberto","ouvert","aperto",
        "communication","comunicaciÃ³n","comunicaciÃ³","comunicaÃ§Ã£o","comunicazione",
        "outreach","divulgaciÃ³n","divulgaciÃ³","divulgaÃ§Ã£o",
        "policy","polÃ­tica","politique","politica",
        "open science","open data","open access","ciencia abierta","ciÃ¨ncia oberta",
        "science communication","science diplomacy","diplomacia cientÃ­fica",
    ]
    urls, seen = [], set()
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        txt = a.get_text(" ").strip().lower()
        full = urljoin(base_url, href)
        if not same_domain(full, base_url):
            continue
        low = (href + " " + txt).lower()
        if any(k in low for k in keys):
            if full not in seen:
                urls.append(full); seen.add(full)
        if len(urls) >= limit:
            break
    return urls

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Google Custom Search (JSON) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def google_cse_search(query: str, per_page: int = 10, pages: int = 3) -> list[dict]:
    results = []
    if not API_KEY or not CSE_CX:
        return results
    per_page = max(1, min(10, per_page))
    for p in range(pages):
        start = 1 + p * per_page
        params = {
            "key": API_KEY,
            "cx": CSE_CX,
            "q": query,
            "num": per_page,
            "start": start,
            "hl": "es",
            "safe": "off",
        }
        try:
            r = requests.get("https://www.googleapis.com/customsearch/v1", params=params, timeout=REQ_TIMEOUT)
            if r.status_code != 200:
                break
            data = r.json()
            for it in data.get("items", []):
                results.append({"link": it.get("link"), "title": it.get("title",""), "snippet": it.get("snippet","")})
            if not data.get("items"):
                break
        except Exception:
            break
        time.sleep(0.3)
    return results

def search_universities_for_category(cat: str, terms: list[str], per_page: int, pages: int) -> list[dict]:
    out, used_domains = [], set()
    for term in terms:
        hits = google_cse_search(term, per_page=per_page, pages=pages)
        for h in hits:
            url, title = h.get("link"), h.get("title","")
            if not url or not is_university(url, title):
                continue
            dom = urlparse(url).netloc.lower()
            if any(bad in dom for bad in BAD_DOMAINS_SUBSTR):
                continue
            if dom in used_domains:
                continue
            used_domains.add(dom)
            out.append({"url": url, "titulo": title, "categoria": cat, "termino": term})
        time.sleep(0.2)
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AnÃ¡lisis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scan_site(url: str, follow_links: int = 3) -> dict:
    result = {
        "url": url, "accesible": False, "idioma": "", "contenido_muestra": "",
        "urls_analizadas": [], "scores": {"ciencia_abierta":0,"comunicacion_publica":0,"diplomacia_cientifica":0},
        "hits": {"ciencia_abierta":[], "comunicacion_publica":[], "diplomacia_cientifica":[]},
    }
    code, html, ct = http_get(url)
    if code != 200:
        return result
    result["accesible"] = True
    text = ""
    if html:  # solo si es HTML/texto
        text = clean_text(html)
        result["contenido_muestra"] = text[:600]
    result["urls_analizadas"].append(url)

    low = text.lower()
    # Enlaces internos (solo si la base era HTML)
    if html:
        links = find_relevant_links(url, html, limit=12)[:follow_links]
        for lk in links:
            c2, h2, ct2 = http_get(lk)
            if c2 == 200 and h2:  # ignorar PDF/binarios
                t2 = clean_text(h2)
                low += " " + t2.lower()
                result["urls_analizadas"].append(lk)
            time.sleep(0.25)

    # Scoring
    for cat, terms in TERMINOS_VALIDACION.items():
        for term in terms:
            tl = term.lower()
            if tl in low:
                m = re.search(r".{0,120}"+re.escape(tl)+r".{0,120}", low, re.DOTALL)
                ctx = m.group(0) if m else tl
                ctx = re.sub(r"\s+", " ", ctx)[:240]
                result["hits"][cat].append({"termino": term, "contexto": ctx})
        result["scores"][cat] = len(result["hits"][cat])
    return result

def force_find_control(control: dict) -> dict:
    res = scan_site(control["url"], follow_links=3)
    if any(v > 0 for v in res["scores"].values()):
        return res
    for q in control["buscar_refuerzo"]:
        hits = google_cse_search(q, per_page=10, pages=2)
        for h in hits:
            u = h.get("link", "")
            if not u or not same_domain(u, control["url"]):
                continue
            c2, h2, ct2 = http_get(u)
            if c2 == 200 and h2:
                t2 = clean_text(h2).lower()
                for cat, terms in TERMINOS_VALIDACION.items():
                    for term in terms:
                        tl = term.lower()
                        if tl in t2:
                            m = re.search(r".{0,120}"+re.escape(tl)+r".{0,120}", t2, re.DOTALL)
                            ctx = m.group(0) if m else tl
                            ctx = re.sub(r"\s+", " ", ctx)[:240]
                            res["hits"][cat].append({"termino": term, "contexto": ctx})
                    res["scores"][cat] = len(res["hits"][cat])
                res["urls_analizadas"].append(u)
                if any(v > 0 for v in res["scores"].values()):
                    return res
    return res

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ” Relevador CPC â€” Universidades con Ciencia Abierta, CPC y Diplomacia CientÃ­fica")
st.caption("Motor: Google Custom Search JSON API (Programmable Search Engine).")
with st.expander("â„¹ï¸ Motor configurado"):
    st.write(f"**CX**: `{CSE_CX or 'NO CONFIGURADO'}` â€” [Ver buscador pÃºblico]({CSE_PUBLIC_URL})")

with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n de bÃºsqueda")
    terms_per_cat = st.slider("TÃ©rminos por categorÃ­a", 3, 12, 6)
    per_page = st.slider("Resultados por pÃ¡gina (1â€“10)", 1, 10, 10)
    pages = st.slider("PÃ¡ginas por tÃ©rmino", 1, 5, 3)
    follow_links = st.slider("Enlaces internos a seguir por sitio", 0, 5, 3)

c1, c2 = st.columns([3,1])
with c1:
    go = st.button("ğŸš€ Ejecutar bÃºsqueda y anÃ¡lisis", type="primary", width="stretch")
with c2:
    clear = st.button("ğŸ—‘ï¸ Limpiar", width="stretch")
if clear:
    st.session_state.pop("results", None)
    st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EjecuciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if go:
    if not API_KEY or not CSE_CX:
        st.error("Falta configurar GOOGLE_API_KEY y/o GOOGLE_CSE_CX en Secrets o variables de entorno.")
    else:
        st.session_state["results"] = []
        progress = st.progress(0.0)
        status = st.empty()
        steps_total = 1 + 3
        step = 0

        # Control
        step += 1
        status.write(f"Analizando poblaciÃ³n de control: {CONTROL['nombre']} ({step}/{steps_total})")
        res_control = force_find_control(CONTROL)
        st.session_state["results"].append({
            "Universidad": CONTROL["nombre"],
            "PaÃ­s": CONTROL["pais"],
            "URL": CONTROL["url"],
            "CategorÃ­a Encontrada": "Control",
            "TÃ©rmino de BÃºsqueda": "",
            "Fecha AnÃ¡lisis": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Sitio Accesible": "SÃ­" if res_control["accesible"] else "No",
            "Idioma Detectado": res_control["idioma"],
            "Ciencia Abierta": "SÃ­" if res_control["scores"]["ciencia_abierta"]>0 else "No",
            "CA - Score": res_control["scores"]["ciencia_abierta"],
            "CA - TÃ©rminos": ", ".join(x["termino"] for x in res_control["hits"]["ciencia_abierta"]),
            "ComunicaciÃ³n PÃºblica": "SÃ­" if res_control["scores"]["comunicacion_publica"]>0 else "No",
            "CP - Score": res_control["scores"]["comunicacion_publica"],
            "CP - TÃ©rminos": ", ".join(x["termino"] for x in res_control["hits"]["comunicacion_publica"]),
            "Diplomacia CientÃ­fica": "SÃ­" if res_control["scores"]["diplomacia_cientifica"]>0 else "No",
            "DC - Score": res_control["scores"]["diplomacia_cientifica"],
            "DC - TÃ©rminos": ", ".join(x["termino"] for x in res_control["hits"]["diplomacia_cientifica"]),
            "URLs Analizadas": "; ".join(res_control["urls_analizadas"]),
            "Contenido Muestra": res_control["contenido_muestra"],
        })
        progress.progress(step/steps_total)

        # CategorÃ­as
        for cat in ["ciencia_abierta","comunicacion_publica","diplomacia_cientifica"]:
            step += 1
            status.write(f"Buscando universidades: {cat.replace('_',' ').title()} ({step}/{steps_total})")
            term_list = TERMINOS_BUSQUEDA[cat][:terms_per_cat]
            candidates = search_universities_for_category(cat, term_list, per_page, pages)
            for c in candidates:
                dom = urlparse(c["url"]).netloc.lower()
                pais = country_guess(dom)
                scan = scan_site(c["url"], follow_links=follow_links)
                st.session_state["results"].append({
                    "Universidad": dom,
                    "PaÃ­s": pais,
                    "URL": c["url"],
                    "CategorÃ­a Encontrada": cat,
                    "TÃ©rmino de BÃºsqueda": c["termino"],
                    "Fecha AnÃ¡lisis": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "Sitio Accesible": "SÃ­" if scan["accesible"] else "No",
                    "Idioma Detectado": scan["idioma"],
                    "Ciencia Abierta": "SÃ­" if scan["scores"]["ciencia_abierta"]>0 else "No",
                    "CA - Score": scan["scores"]["ciencia_abierta"],
                    "CA - TÃ©rminos": ", ".join(x["termino"] for x in scan["hits"]["ciencia_abierta"]),
                    "ComunicaciÃ³n PÃºblica": "SÃ­" if scan["scores"]["comunicacion_publica"]>0 else "No",
                    "CP - Score": scan["scores"]["comunicacion_publica"],
                    "CP - TÃ©rminos": ", ".join(x["termino"] for x in scan["hits"]["comunicacion_publica"]),
                    "Diplomacia CientÃ­fica": "SÃ­" if scan["scores"]["diplomacia_cientifica"]>0 else "No",
                    "DC - Score": scan["scores"]["diplomacia_cientifica"],
                    "DC - TÃ©rminos": ", ".join(x["termino"] for x in scan["hits"]["diplomacia_cientifica"]),
                    "URLs Analizadas": "; ".join(scan["urls_analizadas"]),
                    "Contenido Muestra": scan["contenido_muestra"],
                })
            progress.progress(step/steps_total)

        status.write("âœ… BÃºsqueda finalizada")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PresentaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "results" in st.session_state and st.session_state["results"]:
    st.markdown("---")
    st.subheader("ğŸ“Š Resultados")

    df = pd.DataFrame(st.session_state["results"])

    total = len(df)
    accesibles = (df["Sitio Accesible"]=="SÃ­").sum()
    ca = (df["Ciencia Abierta"]=="SÃ­").sum()
    cp = (df["ComunicaciÃ³n PÃºblica"]=="SÃ­").sum()
    dc = (df["Diplomacia CientÃ­fica"]=="SÃ­").sum()

    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("ğŸ›ï¸ Total", total)
    c2.metric("ğŸŒ Accesibles", f"{accesibles}/{total}")
    c3.metric("ğŸ“Š Ciencia Abierta", ca)
    c4.metric("ğŸ“¢ Com. PÃºblica", cp)
    c5.metric("ğŸ¤ Diplomacia", dc)

    st.dataframe(df[[
        "Universidad","PaÃ­s","CategorÃ­a Encontrada","Sitio Accesible","Idioma Detectado",
        "Ciencia Abierta","ComunicaciÃ³n PÃºblica","Diplomacia CientÃ­fica",
        "CA - Score","CP - Score","DC - Score","URL"
    ]], width="stretch")

    with st.expander("ğŸ” Filtros"):
        cols = st.columns(3)
        with cols[0]:
            paises = ["Todos"] + sorted(df["PaÃ­s"].dropna().unique().tolist())
            f_pais = st.selectbox("PaÃ­s", paises)
        with cols[1]:
            cats = ["Todas","Control","ciencia_abierta","comunicacion_publica","diplomacia_cientifica"]
            f_cat = st.selectbox("CategorÃ­a", cats)
        with cols[2]:
            solo_contenido = st.checkbox("Solo con contenido relevante", value=False)

        filtered = df.copy()
        if f_pais != "Todos":
            filtered = filtered[filtered["PaÃ­s"]==f_pais]
        if f_cat != "Todas":
            filtered = filtered[filtered["CategorÃ­a Encontrada"]==f_cat]
        if solo_contenido:
            filtered = filtered[
                (filtered["Ciencia Abierta"]=="SÃ­") |
                (filtered["ComunicaciÃ³n PÃºblica"]=="SÃ­") |
                (filtered["Diplomacia CientÃ­fica"]=="SÃ­")
            ]

        st.dataframe(filtered, width="stretch")

    st.subheader("ğŸ” Detalle y muestra de contenido")
    for i, row in filtered.head(40).iterrows():
        with st.expander(f"ğŸŒ {row['Universidad']} â€” {row['PaÃ­s']} [{row['CategorÃ­a Encontrada']}]"):
            c1, c2 = st.columns(2)
            with c1:
                st.write(f"**URL:** {row['URL']}")
                st.write(f"**Accesible:** {row['Sitio Accesible']}")
                st.write(f"**Idioma:** {row['Idioma Detectado']}")
            with c2:
                st.write(f"**CA Score:** {row['CA - Score']} â€” {row['CA - TÃ©rminos']}")
                st.write(f"**CP Score:** {row['CP - Score']} â€” {row['CP - TÃ©rminos']}")
                st.write(f"**DC Score:** {row['DC - Score']} â€” {row['DC - TÃ©rminos']}")
            st.text_area("Contenido de la pÃ¡gina", row.get("Contenido Muestra","") or "",
                         height=120, key=f"contenido_{i}", label_visibility="collapsed")

    st.markdown("---")
    st.subheader("ğŸ“¥ Exportar Excel")

    if st.button("ğŸ“Š Generar Excel Completo", width="stretch"):
        with st.spinner("Generando Excel..."):
            df_export = sanitize_dataframe_for_excel(df.copy())
            out = io.BytesIO()
            with pd.ExcelWriter(out, engine="openpyxl") as writer:
                df_export.to_excel(writer, sheet_name="Universidades", index=False)
                resumen = pd.DataFrame([
                    {"MÃ©trica":"Total", "Valor": total},
                    {"MÃ©trica":"Accesibles", "Valor": f"{accesibles}/{total}"},
                    {"MÃ©trica":"Con Ciencia Abierta", "Valor": ca},
                    {"MÃ©trica":"Con ComunicaciÃ³n PÃºblica", "Valor": cp},
                    {"MÃ©trica":"Con Diplomacia CientÃ­fica", "Valor": dc},
                ])
                resumen = sanitize_dataframe_for_excel(resumen)
                resumen.to_excel(writer, sheet_name="Resumen", index=False)
            out.seek(0)
            st.download_button(
                label="â¬‡ï¸ Descargar Relevamiento_CPC.xlsx",
                data=out,
                file_name=f"relevamiento_cpc_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            st.success("âœ… Excel generado")
