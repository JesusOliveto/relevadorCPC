# -*- coding: utf-8 -*-
"""
Relevador CPC (Comunicaci√≥n P√∫blica de la Ciencia)
--------------------------------------------------
- Busca universidades con Google Custom Search JSON API (Programmable Search Engine).
- Categor√≠as: Ciencia Abierta, Comunicaci√≥n P√∫blica de la Ciencia, Diplomacia Cient√≠fica.
- Analiza p√°ginas (home + enlaces internos relevantes), detecta t√©rminos por categor√≠a y exporta Excel.
- Compatible con Streamlit Cloud. Ajustes deprecados: width='stretch' y labels accesibles.

Autor: Sistema de Relevamiento CPC
Fecha: Septiembre 2025
"""

import os
import io
import re
import time
import json
import random
import requests
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, quote_plus
from datetime import datetime

# ==============================
# Config. Streamlit
# ==============================
st.set_page_config(
    page_title="Relevador CPC - Buscador de Universidades",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ==============================
# Claves (Secrets / Entorno / Fallback)
# ==============================
API_KEY = (
    (st.secrets.get("GOOGLE_API_KEY") if hasattr(st, "secrets") else None)
    or os.getenv("GOOGLE_API_KEY")
)

CSE_CX = (
    (st.secrets.get("GOOGLE_CSE_CX") if hasattr(st, "secrets") else None)
    or os.getenv("GOOGLE_CSE_CX")
)

# URL p√∫blica del buscador (informativa)
CSE_PUBLIC_URL = "https://cse.google.com/cse?cx=d15e41407a20a49c6"

# ==============================
# Constantes y vocabularios
# ==============================
REQ_TIMEOUT = 12
HEADERS_POOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
]
def _headers():
    return {"User-Agent": random.choice(HEADERS_POOL), "Accept-Language": "es-AR,es;q=0.9,en;q=0.8"}

TERMINOS_BUSQUEDA = {
    "ciencia_abierta": [
        '("open science" OR "open data" OR "open access") university',
        "universidad ciencia abierta",
        "universitat ci√®ncia oberta",
        "universit√© science ouverte",
        "universidade ci√™ncia aberta",
        "universit√† scienza aperta",
    ],
    "comunicacion_publica": [
        '("science communication" OR "public engagement") university',
        "universidad comunicaci√≥n p√∫blica de la ciencia",
        "universitat comunicaci√≥ cient√≠fica",
        "universit√© communication scientifique",
        "universidade comunica√ß√£o cient√≠fica",
    ],
    "diplomacia_cientifica": [
        '("science diplomacy") university',
        "universidad diplomacia cient√≠fica",
        "universitat diplom√†cia cient√≠fica",
        "universit√© diplomatie scientifique",
        "universidade diplomacia cient√≠fica",
    ],
}

TERMINOS_VALIDACION = {
    "ciencia_abierta": [
        "open science", "ciencia abierta", "ci√®ncia oberta", "ci√™ncia aberta", "science ouverte",
        "open data", "datos abiertos", "dades obertes", "dados abertos", "donn√©es ouvertes",
        "open access", "acceso abierto", "acc√©s obert", "acesso aberto", "acc√®s libre",
        "fair data", "repositorio institucional", "institutional repository", "reproducible research",
    ],
    "comunicacion_publica": [
        "science communication", "comunicaci√≥n cient√≠fica", "comunicaci√≥n p√∫blica de la ciencia",
        "public engagement", "outreach", "vulgarisation scientifique", "divulga√ß√£o cient√≠fica",
        "science literacy", "public understanding of science", "cultura cient√≠fica",
    ],
    "diplomacia_cientifica": [
        "science diplomacy", "diplomacia cient√≠fica", "diplom√†cia cient√≠fica", "diplomatie scientifique",
        "international scientific cooperation", "cooperaci√≥n internacional cient√≠fica",
        "science policy", "pol√≠tica cient√≠fica",
    ],
}

# Dominios a excluir (rankings/directorios/etc.)
BAD_DOMAINS_SUBSTR = [
    "wikipedia.org", "wikidata.org", "web.archive.org",
    "blogspot.", "wordpress.", "medium.com",
    "topuniversities.", "timeshighereducation.", "theworlduniversityrankings",
    "4icu.org", "uni-rank", "edurank", "shanghairanking",
    "qs.com", "mastersportal", "bachelorstudies", "studocu", "prezi",
    "/ranking", "/rankings", "/directory", "/directorio",
]

CONTROL = {
    "nombre": "Universitat Jaume I",
    "url": "https://www.uji.es/",
    "pais": "Espa√±a",
    "idioma": "Catal√°n/Espa√±ol",
    "buscar_refuerzo": ['site:uji.es ("ci√®ncia oberta" OR "ciencia abierta" OR "open science")'],
}

# ==============================
# Utilidades
# ==============================
def clean_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for bad in soup(["script", "style", "noscript", "header", "footer", "nav"]):
        bad.decompose()
    text = soup.get_text(separator=" ")
    return re.sub(r"\s+", " ", text).strip()

@st.cache_data(show_spinner=False, ttl=24*3600)
def http_get(url: str) -> tuple[int, str]:
    try:
        r = requests.get(url, headers=_headers(), timeout=REQ_TIMEOUT)
        return r.status_code, r.text
    except Exception:
        return 0, ""

def same_domain(a: str, b: str) -> bool:
    try:
        return urlparse(a).netloc == urlparse(b).netloc
    except:
        return False

def is_university(url: str, title: str = "") -> bool:
    if not url:
        return False
    u = url.lower()
    # filtrar directorios/rankings
    if any(b in u for b in BAD_DOMAINS_SUBSTR):
        return False
    # se√±ales fuertes por dominio
    strong = any(re.search(p, u) for p in [
        r"\.edu($|/)", r"\.edu\.[a-z]+$", r"\.ac\.", r"\.ac$",
        r"\.uni\.", r"\.univ\.", r"\.edu\.", r"\.ac\.[a-z]+$"
    ])
    if strong:
        return True
    # t√≠tulo/texto
    t = (title or "").lower()
    words = ["university","universidad","universitat","universidade","universit√©","universit√†","college","instituto","institute"]
    if any(w in t for w in words) and not any(x in t for x in ["ranking","rankings","list","directorio","directory","top"]):
        return True
    # hostname con 'uni.' o 'univ.'
    host = urlparse(url).netloc.lower()
    if host.startswith("uni.") or host.startswith("univ."):
        return True
    return False

def country_guess(domain: str) -> str:
    m = {
        ".es":"Espa√±a",".edu":"Estados Unidos",".uk":"Reino Unido",".ca":"Canad√°",".au":"Australia",".de":"Alemania",
        ".fr":"Francia",".it":"Italia",".br":"Brasil",".ar":"Argentina",".mx":"M√©xico",".cl":"Chile",".co":"Colombia",
        ".pe":"Per√∫",".jp":"Jap√≥n",".cn":"China",".in":"India",".nl":"Pa√≠ses Bajos",".ch":"Suiza",".se":"Suecia",".no":"Noruega"
    }
    for ext, pais in m.items():
        if domain.endswith(ext): return pais
    return "Internacional"

def find_relevant_links(base_url: str, html: str, limit: int = 12) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")
    keys = [
        "research","investigaci√≥n","investigaci√≥","pesquisa","recherche","ricerca",
        "science","ciencia","ci√®ncia","ci√™ncia","scienza",
        "open","abierto","obert","aberto","ouvert","aperto",
        "communication","comunicaci√≥n","comunicaci√≥","comunica√ß√£o","comunicazione",
        "outreach","divulgaci√≥n","divulgaci√≥","divulga√ß√£o",
        "policy","pol√≠tica","politique","politica",
        "open science","open data","open access","ciencia abierta","ci√®ncia oberta",
        "science communication","science diplomacy","diplomacia cient√≠fica",
    ]
    urls, seen = [], set()
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        txt = a.get_text(" ").strip().lower()
        full = urljoin(base_url, href)
        if not same_domain(full, base_url):
            continue
        low = (href + " " + txt).lower()
        if any(k in low for k in keys):
            if full not in seen:
                urls.append(full); seen.add(full)
        if len(urls) >= limit:
            break
    return urls

# ==============================
# Google Custom Search (JSON API)
# ==============================
def google_cse_search(query: str, per_page: int = 10, pages: int = 3) -> list[dict]:
    """
    Devuelve lista de resultados: dict con keys: link, title, snippet.
    """
    results = []
    per_page = max(1, min(10, per_page))  # API permite num=1..10
    for p in range(pages):
        start = 1 + p * per_page  # start: 1-based
        params = {
            "key": API_KEY,
            "cx": CSE_CX,
            "q": query,
            "num": per_page,
            "start": start,
            "hl": "es",
            "safe": "off",
        }
        try:
            r = requests.get("https://www.googleapis.com/customsearch/v1", params=params, timeout=REQ_TIMEOUT)
            if r.status_code != 200:
                # 403/429 por cuota ‚Üí salir temprano
                break
            data = r.json()
            items = data.get("items", [])
            for it in items:
                results.append({
                    "link": it.get("link"),
                    "title": it.get("title", ""),
                    "snippet": it.get("snippet", ""),
                })
            # Si no trajo nada, cortar
            if not items:
                break
        except Exception:
            break
        time.sleep(0.3)  # ser cort√©s con la API
    return results

def search_universities_for_category(cat: str, terms: list[str], per_page: int, pages: int) -> list[dict]:
    out, used_domains = [], set()
    for term in terms:
        hits = google_cse_search(term, per_page=per_page, pages=pages)
        for h in hits:
            url, title = h.get("link"), h.get("title","")
            if not url:
                continue
            if not is_university(url, title):
                continue
            dom = urlparse(url).netloc.lower()
            if any(bad in dom for bad in BAD_DOMAINS_SUBSTR):
                continue
            if dom in used_domains:
                continue
            used_domains.add(dom)
            out.append({"url": url, "titulo": title, "categoria": cat, "termino": term})
        time.sleep(0.2)
    return out

# ==============================
# An√°lisis de sitio
# ==============================
def scan_site(url: str, follow_links: int = 3) -> dict:
    result = {
        "url": url, "accesible": False, "idioma": "", "contenido_muestra": "",
        "urls_analizadas": [], "scores": {"ciencia_abierta":0,"comunicacion_publica":0,"diplomacia_cientifica":0},
        "hits": {"ciencia_abierta":[], "comunicacion_publica":[], "diplomacia_cientifica":[]},
    }
    code, html = http_get(url)
    if code != 200 or not html:
        return result
    result["accesible"] = True
    text = clean_text(html)
    result["contenido_muestra"] = text[:600]
    result["urls_analizadas"].append(url)

    # idioma simple por palabras frecuentes
    low = text.lower()
    lang_scores = {
        "espa√±ol": sum(w in low for w in ["universidad","investigaci√≥n","ciencia","estudiantes","facultad"]),
        "catal√°n": sum(w in low for w in ["universitat","investigaci√≥","ci√®ncia","estudiants","facultat"]),
        "ingl√©s":  sum(w in low for w in ["university","research","science","students","faculty"]),
        "portugu√©s": sum(w in low for w in ["universidade","pesquisa","ci√™ncia","estudantes","faculdade"]),
        "franc√©s": sum(w in low for w in ["universit√©","recherche","science","√©tudiants","facult√©"]),
        "italiano": sum(w in low for w in ["universit√†","ricerca","scienza","studenti","facolt√†"]),
    }
    result["idioma"] = max(lang_scores, key=lang_scores.get) if any(lang_scores.values()) else "Desconocido"

    # Seguir enlaces internos relevantes
    links = find_relevant_links(url, html, limit=12)[:follow_links]
    for lk in links:
        c2, h2 = http_get(lk)
        if c2 == 200 and h2:
            t2 = clean_text(h2)
            low += " " + t2.lower()
            result["urls_analizadas"].append(lk)
        time.sleep(0.25)

    # Scoring por categor√≠a
    for cat, terms in TERMINOS_VALIDACION.items():
        for term in terms:
            tl = term.lower()
            if tl in low:
                m = re.search(r".{0,120}"+re.escape(tl)+r".{0,120}", low, re.DOTALL)
                ctx = m.group(0) if m else tl
                ctx = re.sub(r"\s+", " ", ctx)[:240]
                result["hits"][cat].append({"termino": term, "contexto": ctx})
        result["scores"][cat] = len(result["hits"][cat])
    return result

def force_find_control(control: dict) -> dict:
    """
    Si la home de UJI no muestra t√©rminos (por banners), refuerza con b√∫squeda site:.
    """
    res = scan_site(control["url"], follow_links=3)
    if any(v > 0 for v in res["scores"].values()):
        return res
    # B√∫squeda espec√≠fica dentro del dominio
    for q in control["buscar_refuerzo"]:
        hits = google_cse_search(q, per_page=10, pages=2)
        for h in hits:
            u = h.get("link", "")
            if not u or not same_domain(u, control["url"]):
                continue
            c2, h2 = http_get(u)
            if c2 == 200 and h2:
                t2 = clean_text(h2).lower()
                for cat, terms in TERMINOS_VALIDACION.items():
                    for term in terms:
                        tl = term.lower()
                        if tl in t2:
                            m = re.search(r".{0,120}"+re.escape(tl)+r".{0,120}", t2, re.DOTALL)
                            ctx = m.group(0) if m else tl
                            ctx = re.sub(r"\s+", " ", ctx)[:240]
                            res["hits"][cat].append({"termino": term, "contexto": ctx})
                    res["scores"][cat] = len(res["hits"][cat])
                res["urls_analizadas"].append(u)
                if any(v > 0 for v in res["scores"].values()):
                    return res
    return res

# ==============================
# UI
# ==============================
st.title("üîç Relevador CPC ‚Äî Universidades con Ciencia Abierta, CPC y Diplomacia Cient√≠fica")
st.caption("Motor: Google Custom Search JSON API (Programmable Search Engine).")
with st.expander("‚ÑπÔ∏è Motor configurado"):
    st.write(f"**CX**: `{CSE_CX}` ‚Äî [Ver buscador p√∫blico]({CSE_PUBLIC_URL})")

with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n de b√∫squeda")
    terms_per_cat = st.slider("T√©rminos por categor√≠a", 3, 12, 6)
    per_page = st.slider("Resultados por p√°gina (CSE permite 1‚Äì10)", 1, 10, 10)
    pages = st.slider("P√°ginas por t√©rmino", 1, 5, 3)
    follow_links = st.slider("Enlaces internos a seguir por sitio", 0, 5, 3)

# Acciones
c1, c2 = st.columns([3,1])
with c1:
    go = st.button("üöÄ Ejecutar b√∫squeda y an√°lisis", type="primary", width="stretch")
with c2:
    clear = st.button("üóëÔ∏è Limpiar", width="stretch")

if clear:
    st.session_state.pop("results", None)
    st.rerun()

# ==============================
# Ejecuci√≥n
# ==============================
if go:
    if not API_KEY or not CSE_CX:
        st.error("Falta configurar GOOGLE_API_KEY y/o GOOGLE_CSE_CX. Cargalos en Secrets o variables de entorno.")
    else:
        st.session_state["results"] = []
        progress = st.progress(0.0)
        status = st.empty()
        steps_total = 1 + 3  # control + 3 categor√≠as
        step = 0

        # 1) Control: UJI
        step += 1
        status.write(f"Analizando poblaci√≥n de control: {CONTROL['nombre']} ({step}/{steps_total})")
        res_control = force_find_control(CONTROL)

        uni_row = {
            "Universidad": CONTROL["nombre"],
            "Pa√≠s": CONTROL["pais"],
            "URL": CONTROL["url"],
            "Categor√≠a Encontrada": "Control",
            "T√©rmino de B√∫squeda": "",
            "Fecha An√°lisis": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Sitio Accesible": "S√≠" if res_control["accesible"] else "No",
            "Idioma Detectado": res_control["idioma"],
            "Ciencia Abierta": "S√≠" if res_control["scores"]["ciencia_abierta"]>0 else "No",
            "CA - Score": res_control["scores"]["ciencia_abierta"],
            "CA - T√©rminos": ", ".join(x["termino"] for x in res_control["hits"]["ciencia_abierta"]),
            "Comunicaci√≥n P√∫blica": "S√≠" if res_control["scores"]["comunicacion_publica"]>0 else "No",
            "CP - Score": res_control["scores"]["comunicacion_publica"],
            "CP - T√©rminos": ", ".join(x["termino"] for x in res_control["hits"]["comunicacion_publica"]),
            "Diplomacia Cient√≠fica": "S√≠" if res_control["scores"]["diplomacia_cientifica"]>0 else "No",
            "DC - Score": res_control["scores"]["diplomacia_cientifica"],
            "DC - T√©rminos": ", ".join(x["termino"] for x in res_control["hits"]["diplomacia_cientifica"]),
            "URLs Analizadas": "; ".join(res_control["urls_analizadas"]),
            "Contenido Muestra": res_control["contenido_muestra"],
        }
        st.session_state["results"].append(uni_row)
        progress.progress(step/steps_total)

        # 2) Categor√≠as
        for cat in ["ciencia_abierta", "comunicacion_publica", "diplomacia_cientifica"]:
            step += 1
            status.write(f"Buscando universidades: {cat.replace('_',' ').title()} ({step}/{steps_total})")
            term_list = TERMINOS_BUSQUEDA[cat][:terms_per_cat]
            candidates = search_universities_for_category(cat, term_list, per_page, pages)

            for c in candidates:
                dom = urlparse(c["url"]).netloc.lower()
                pais = country_guess(dom)
                scan = scan_site(c["url"], follow_links=follow_links)
                row = {
                    "Universidad": dom,
                    "Pa√≠s": pais,
                    "URL": c["url"],
                    "Categor√≠a Encontrada": cat,
                    "T√©rmino de B√∫squeda": c["termino"],
                    "Fecha An√°lisis": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "Sitio Accesible": "S√≠" if scan["accesible"] else "No",
                    "Idioma Detectado": scan["idioma"],
                    "Ciencia Abierta": "S√≠" if scan["scores"]["ciencia_abierta"]>0 else "No",
                    "CA - Score": scan["scores"]["ciencia_abierta"],
                    "CA - T√©rminos": ", ".join(x["termino"] for x in scan["hits"]["ciencia_abierta"]),
                    "Comunicaci√≥n P√∫blica": "S√≠" if scan["scores"]["comunicacion_publica"]>0 else "No",
                    "CP - Score": scan["scores"]["comunicacion_publica"],
                    "CP - T√©rminos": ", ".join(x["termino"] for x in scan["hits"]["comunicacion_publica"]),
                    "Diplomacia Cient√≠fica": "S√≠" if scan["scores"]["diplomacia_cientifica"]>0 else "No",
                    "DC - Score": scan["scores"]["diplomacia_cientifica"],
                    "DC - T√©rminos": ", ".join(x["termino"] for x in scan["hits"]["diplomacia_cientifica"]),
                    "URLs Analizadas": "; ".join(scan["urls_analizadas"]),
                    "Contenido Muestra": scan["contenido_muestra"],
                }
                st.session_state["results"].append(row)
            progress.progress(step/steps_total)

        status.write("‚úÖ B√∫squeda finalizada")

# ==============================
# Presentaci√≥n resultados
# ==============================
if "results" in st.session_state and st.session_state["results"]:
    st.markdown("---")
    st.subheader("üìä Resultados")

    df = pd.DataFrame(st.session_state["results"])

    total = len(df)
    accesibles = (df["Sitio Accesible"]=="S√≠").sum()
    ca = (df["Ciencia Abierta"]=="S√≠").sum()
    cp = (df["Comunicaci√≥n P√∫blica"]=="S√≠").sum()
    dc = (df["Diplomacia Cient√≠fica"]=="S√≠").sum()

    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("üèõÔ∏è Total", total)
    c2.metric("üåê Accesibles", f"{accesibles}/{total}")
    c3.metric("üìä Ciencia Abierta", ca)
    c4.metric("üì¢ Com. P√∫blica", cp)
    c5.metric("ü§ù Diplomacia", dc)

    st.dataframe(df[[
        "Universidad","Pa√≠s","Categor√≠a Encontrada","Sitio Accesible","Idioma Detectado",
        "Ciencia Abierta","Comunicaci√≥n P√∫blica","Diplomacia Cient√≠fica",
        "CA - Score","CP - Score","DC - Score","URL"
    ]], width="stretch")

    with st.expander("üîç Filtros"):
        cols = st.columns(3)
        with cols[0]:
            paises = ["Todos"] + sorted(df["Pa√≠s"].dropna().unique().tolist())
            f_pais = st.selectbox("Pa√≠s", paises)
        with cols[1]:
            cats = ["Todas","Control","ciencia_abierta","comunicacion_publica","diplomacia_cientifica"]
            f_cat = st.selectbox("Categor√≠a", cats)
        with cols[2]:
            solo_contenido = st.checkbox("Solo con contenido relevante", value=False)

        filtered = df.copy()
        if f_pais != "Todos":
            filtered = filtered[filtered["Pa√≠s"]==f_pais]
        if f_cat != "Todas":
            filtered = filtered[filtered["Categor√≠a Encontrada"]==f_cat]
        if solo_contenido:
            filtered = filtered[
                (filtered["Ciencia Abierta"]=="S√≠") |
                (filtered["Comunicaci√≥n P√∫blica"]=="S√≠") |
                (filtered["Diplomacia Cient√≠fica"]=="S√≠")
            ]

        st.dataframe(filtered, width="stretch")

    st.subheader("üîé Detalle y muestra de contenido")
    for i, row in filtered.head(40).iterrows():
        with st.expander(f"üåê {row['Universidad']} ‚Äî {row['Pa√≠s']} [{row['Categor√≠a Encontrada']}]"):
            c1, c2 = st.columns(2)
            with c1:
                st.write(f"**URL:** {row['URL']}")
                st.write(f"**Accesible:** {row['Sitio Accesible']}")
                st.write(f"**Idioma:** {row['Idioma Detectado']}")
            with c2:
                st.write(f"**CA Score:** {row['CA - Score']} ‚Äî {row['CA - T√©rminos']}")
                st.write(f"**CP Score:** {row['CP - Score']} ‚Äî {row['CP - T√©rminos']}")
                st.write(f"**DC Score:** {row['DC - Score']} ‚Äî {row['DC - T√©rminos']}")
            # label no vac√≠o + oculto
            st.text_area(
                "Contenido de la p√°gina",
                row.get("Contenido Muestra","") or "",
                height=120,
                key=f"contenido_{i}",
                label_visibility="collapsed"
            )

    st.markdown("---")
    st.subheader("üì• Exportar Excel")

    if st.button("üìä Generar Excel Completo", width="stretch"):
        with st.spinner("Generando Excel..."):
            out = io.BytesIO()
            with pd.ExcelWriter(out, engine="openpyxl") as writer:
                df.to_excel(writer, sheet_name="Universidades", index=False)
                resumen = pd.DataFrame([
                    {"M√©trica":"Total", "Valor": total},
                    {"M√©trica":"Accesibles", "Valor": f"{accesibles}/{total}"},
                    {"M√©trica":"Con Ciencia Abierta", "Valor": ca},
                    {"M√©trica":"Con Comunicaci√≥n P√∫blica", "Valor": cp},
                    {"M√©trica":"Con Diplomacia Cient√≠fica", "Valor": dc},
                ])
                resumen.to_excel(writer, sheet_name="Resumen", index=False)
            out.seek(0)
            st.download_button(
                label="‚¨áÔ∏è Descargar Relevamiento_CPC.xlsx",
                data=out,
                file_name=f"relevamiento_cpc_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            st.success("‚úÖ Listo")